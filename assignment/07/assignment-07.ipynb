{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment-07\n",
    "## < Logistic regression for a binary classification with a regularization>\n",
    "### -20154521 Seokjun Choi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings#ignore warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data    = np.genfromtxt(\"data-nonlinear.txt\", delimiter=',')\n",
    "\n",
    "pointX  = data[:, 0:1]\n",
    "pointY  = data[:, 1:2]\n",
    "label   = data[:, 2:3]\n",
    "\n",
    "pointX0 = pointX[label == 0]\n",
    "pointY0 = pointY[label == 0]\n",
    "\n",
    "pointX1 = pointX[label == 1]\n",
    "pointY1 = pointY[label == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of the traing data\n",
    "m = len(pointX)\n",
    "\n",
    "#zero_term = np.ones((1,m))\n",
    "x = pointX.T\n",
    "y = pointY.T\n",
    "\n",
    "feature = np.ones((1,m))\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        temp_data = (x**(i))*(y**(j))\n",
    "        feature = np.concatenate((feature, temp_data), axis=0)\n",
    "        #print(\"x\", i, \"y\", j)\n",
    "        \n",
    "feature = feature[1:,:]\n",
    "\n",
    "label = label.T\n",
    "\n",
    "learning_rate = 10\n",
    "lamda1 = 0.00001\n",
    "lamda2 = 1\n",
    "lamda3 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overfitting\n",
    "#initiate theata\n",
    "theta1 = np. ones((100, 1))\n",
    "#variable that count iteration\n",
    "t1 = 0\n",
    "theta_list1 = list()\n",
    "theta_list1.append(theta1)\n",
    "iteration1 = list()\n",
    "#list that store j values at every iteration\n",
    "temp_j_value1 = 0\n",
    "j_value1 = list()\n",
    "#list for storing accuracy\n",
    "accuracy_list1 = list()\n",
    "theta1[0,0]=20\n",
    "theta1[3,0]=10\n",
    "theta1[21,0]=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    g = (theta1.T).dot(feature)\n",
    "    sigmoid = 1 / (1 + np.exp(-g))\n",
    "    #if there is no change in j value, then break\n",
    "    if t1>1 and abs(temp_j_value1 - (-(np.sum((label*np.log(sigmoid))+(1-label)*np.log(1-sigmoid)))/m)) <= 0.00000068:\n",
    "        fin_theta1 = theta1\n",
    "        break\n",
    "    temp_j_value1 = -(np.sum((label*np.log(sigmoid))+(1-label)*np.log(1-sigmoid)))/m\n",
    "    j_value1.append(temp_j_value1)\n",
    "    iteration1.append(t1)\n",
    "    #print(t, temp_j_value, '\\n')\n",
    "    \n",
    "    #store at theta list\n",
    "    #for calculating accuracy\n",
    "    \n",
    "    pred0 = (theta1.T).dot(feature)\n",
    "    pred1 = 1 / (1 + np.exp(-pred0))\n",
    "    \n",
    "    for i in range(118):\n",
    "        if (pred1[0, i]>=0.5):\n",
    "            pred1[0, i]=1\n",
    "        else:\n",
    "            pred1[0, i]=0\n",
    "    correct = 0\n",
    "    for i in range(118):\n",
    "        if (pred1[0, i]==label[0,i]):\n",
    "            correct +=1\n",
    "    accuracy = correct / 118\n",
    "    accuracy_list1.append(accuracy)\n",
    "    \n",
    "    \n",
    "    #updata theta\n",
    "    theta_list1.append(theta1)\n",
    "    theta1 = (1-(learning_rate*lamda1)/m)*theta1 - learning_rate * feature.dot((sigmoid-label).T) / m\n",
    "    t1 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just right\n",
    "#initiate theata\n",
    "learning_rate = 3\n",
    "theta2 = np. ones((100, 1))\n",
    "#variable that count iteration\n",
    "t2 = 0\n",
    "theta_list2 = list()\n",
    "theta_list2.append(theta2)\n",
    "iteration2 = list()\n",
    "#list that store j values at every iteration\n",
    "temp_j_value2 = 0\n",
    "j_value2 = list()\n",
    "#list for storing accuracy\n",
    "accuracy_list2 = list()\n",
    "theta2[0,0]=20\n",
    "theta2[3,0]=10\n",
    "theta2[21,0]=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    g = (theta2.T).dot(feature)\n",
    "    sigmoid = 1 / (1 + np.exp(-g))\n",
    "    #if there is no change in j value, then break\n",
    "    if t2>1 and abs(temp_j_value2 - (-(np.sum((label*np.log(sigmoid))+(1-label)*np.log(1-sigmoid)))/m)) == 0:\n",
    "        fin_theta2 = theta2\n",
    "        break\n",
    "    temp_j_value2 = -(np.sum((label*np.log(sigmoid))+(1-label)*np.log(1-sigmoid)))/m\n",
    "    j_value2.append(temp_j_value2)\n",
    "    iteration2.append(t2)\n",
    "    #print(t, temp_j_value, '\\n')\n",
    "    \n",
    "    #store at theta list\n",
    "    #for calculating accuracy\n",
    "    \n",
    "    pred0 = (theta2.T).dot(feature)\n",
    "    pred1 = 1 / (1 + np.exp(-pred0))\n",
    "    \n",
    "    for i in range(118):\n",
    "        if (pred1[0, i]>=0.5):\n",
    "            pred1[0, i]=1\n",
    "        else:\n",
    "            pred1[0, i]=0\n",
    "    correct = 0\n",
    "    for i in range(118):\n",
    "        if (pred1[0, i]==label[0,i]):\n",
    "            correct +=1\n",
    "    accuracy = correct / 118\n",
    "    accuracy_list2.append(accuracy)\n",
    "    \n",
    "    \n",
    "    #updata theta\n",
    "    theta_list2.append(theta2)\n",
    "    theta2 = (1-(learning_rate*lamda2)/m)*theta2 - learning_rate * feature.dot((sigmoid-label).T) / m\n",
    "    t2 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#underfitting\n",
    "#initiate theata\n",
    "theta3 = np. ones((100, 1))\n",
    "#variable that count iteration\n",
    "t3 = 0\n",
    "theta_list3 = list()\n",
    "theta_list3.append(theta3)\n",
    "iteration3 = list()\n",
    "#list that store j values at every iteration\n",
    "temp_j_value3 = 0\n",
    "j_value3 = list()\n",
    "#list for storing accuracy\n",
    "accuracy_list3 = list()\n",
    "theta3[0,0]=20\n",
    "theta3[3,0]=10\n",
    "theta3[21,0]=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    g = (theta3.T).dot(feature)\n",
    "    sigmoid = 1 / (1 + np.exp(-g))\n",
    "    #if there is no change in j value, then break\n",
    "    if t3>1 and abs(temp_j_value3 - (-(np.sum((label*np.log(sigmoid))+(1-label)*np.log(1-sigmoid)))/m)) == 0:\n",
    "        fin_theta3 = theta3\n",
    "        break\n",
    "    temp_j_value3 = -(np.sum((label*np.log(sigmoid))+(1-label)*np.log(1-sigmoid)))/m\n",
    "    j_value3.append(temp_j_value3)\n",
    "    iteration3.append(t3)\n",
    "    #print(t, temp_j_value, '\\n')\n",
    "    \n",
    "    #store at theta list\n",
    "    #for calculating accuracy\n",
    "    \n",
    "    pred0 = (theta3.T).dot(feature)\n",
    "    pred1 = 1 / (1 + np.exp(-pred0))\n",
    "    \n",
    "    for i in range(118):\n",
    "        if (pred1[0, i]>=0.5):\n",
    "            pred1[0, i]=1\n",
    "        else:\n",
    "            pred1[0, i]=0\n",
    "    correct = 0\n",
    "    for i in range(118):\n",
    "        if (pred1[0, i]==label[0,i]):\n",
    "            correct +=1\n",
    "    accuracy = correct / 118\n",
    "    accuracy_list3.append(accuracy)\n",
    "    \n",
    "    \n",
    "    #updata theta\n",
    "    theta_list3.append(theta3)\n",
    "    theta3 = (1-(learning_rate*lamda3)/m)*theta3 - learning_rate * feature.dot((sigmoid-label).T) / m\n",
    "    t3 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate three final accuracy\n",
    "df1 = (fin_theta1.T).dot(feature)\n",
    "sg1 = 1 / (1 + np.exp(-df1))\n",
    "for i in range(118):\n",
    "    if (sg1[0,i]>=0.5):\n",
    "        sg1[0,i]=1\n",
    "    else:\n",
    "        sg1[0,i]=0\n",
    "sum1 = 0\n",
    "for i in range(118):\n",
    "    if (sg1[0,i]==label[0,i]):\n",
    "        sum1 +=1\n",
    "\n",
    "df2 = (fin_theta2.T).dot(feature)\n",
    "sg2 = 1 / (1 + np.exp(-df2))\n",
    "for i in range(118):\n",
    "    if (sg2[0,i]>=0.5):\n",
    "        sg2[0,i]=1\n",
    "    else:\n",
    "        sg2[0,i]=0\n",
    "sum2 = 0\n",
    "for i in range(118):\n",
    "    if (sg2[0,i]==label[0,i]):\n",
    "        sum2 +=1\n",
    "\n",
    "df3 = (fin_theta3.T).dot(feature)\n",
    "sg3 = 1 / (1 + np.exp(-df3))\n",
    "for i in range(118):\n",
    "    if (sg3[0,i]>=0.5):\n",
    "        sg3[0,i]=1\n",
    "    else:\n",
    "        sg3[0,i]=0\n",
    "sum3 = 0\n",
    "for i in range(118):\n",
    "    if (sg3[0,i]==label[0,i]):\n",
    "        sum3 +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#theses are used for plotting\n",
    "xlin = np.arange(-2, 2, 0.01)\n",
    "ylin = np.arange(-2, 2, 0.01)\n",
    "X, Y = np.meshgrid(xlin, ylin)\n",
    "theta_num1 = 0\n",
    "Z1 = np.zeros((400,400))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        Z1 += theta1[theta_num1, 0]*((X**(i))*(Y**(j)))\n",
    "        theta_num1 += 1\n",
    "sigmoids1 = 1 / (1 + np.exp(-Z1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_num2 = 0\n",
    "Z2 = np.zeros((400,400))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        Z2 += theta2[theta_num2, 0]*((X**(i))*(Y**(j)))\n",
    "        theta_num2 += 1\n",
    "sigmoids2 = 1 / (1 + np.exp(-Z2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_num3 = 0\n",
    "Z3 = np.zeros((400,400))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        Z3 += theta3[theta_num3, 0]*((X**(i))*(Y**(j)))\n",
    "        theta_num3 += 1\n",
    "sigmoids3 = 1 / (1 + np.exp(-Z3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Plotting the training data set\n",
    "    - Load data from 'data-nonlinear.txt' file.\n",
    "    - Split data into points, labels.\n",
    "    - Use Scatter() function to plot the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(pointX0, pointY0, c='b')\n",
    "plt.scatter(pointX1, pointY1, c='r')\n",
    "plt.tight_layout()\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('1. The training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient descent\n",
    "    - The cost function is \n",
    "    $J(\\theta)$ = $\\frac{-1}{m}\\sum_{i=1}^{m}(l^{(i)}log(\\sigma(z^{(i)}))$ + $(1 - l^{(i)})log(1 -\\sigma(z^{(i)}))) + \\frac{\\lambda}{2}\\sum_{i=0}^{9}\\sum_{j=0}^{9}\\theta_{i,j}^{2} $.\n",
    "    - '$\\sigma(z)$' is sigmoid function. $\\sigma(z) = \\frac{1}{1+e^{(-z)}}$.\n",
    "    - $z$ is $\\theta^{T}X$$ = \\sum_{i=0}^{9}\\sum_{j=0}^{9}\\theta_{i,j}^{2}x^{i}y^{j}$.\n",
    "    - $l$ is label.\n",
    "    - $\\theta^{(t+1)}$ := $(1 - \\alpha\\frac{\\lambda}{m})\\theta^{(t)}$ - $\\alpha\\frac{1}{m}$$\\sum_{i=1}^{m}{X (\\sigma(\\theta^{T}X) - l)}$.\n",
    "    - Store parameters $\\theta$ and cost J to plot later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Plotting the training error.\n",
    "    - The energy(cost) converges at certain value which is the optimal of our cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(iteration1, j_value1, c = 'red')\n",
    "plt.plot(iteration2, j_value2, c = 'green')\n",
    "plt.plot(iteration3, j_value3, c = 'blue')\n",
    "plt.legend(['over-fitting', 'just-right', 'under-fitting'])\n",
    "plt.xlabel('t : iteration')\n",
    "plt.ylabel('cost value')\n",
    "plt.title('2. The training error')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Display the values of the chosen regularization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_RED    = \"\\033[31m\"\n",
    "C_GREEN  = \"\\033[32m\"\n",
    "C_BLUE   = \"\\033[34m\"\n",
    "print(C_RED + \"lamda1 is \" + str(lamda1))\n",
    "print(C_GREEN + \"lamda2 is \" + str(lamda2))\n",
    "print(C_BLUE + \"lamda3 is \" + str(lamda3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Plotting the training accuracy.\n",
    "    - The accuracy changes at every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(iteration1, accuracy_list1, c = 'red')\n",
    "plt.plot(iteration2, accuracy_list2, c = 'green')\n",
    "plt.plot(iteration3, accuracy_list3, c = 'blue')\n",
    "plt.legend(['over-fitting', 'just-right', 'under-fitting'])\n",
    "plt.xlabel('t : iteration')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('4. The training accuracy')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Writing down the final training accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(C_RED + \"over-fitting accuracy : \" + str((sum1/m)*100) + \"%\")\n",
    "print(C_GREEN + \"just-right accuracy : \" + str((sum2/m)*100) + \"%\")\n",
    "print(C_BLUE + \"under-fitting accuracy : \" + str((sum3/m)*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Plotting the optimal classifier superimposed on the training data\n",
    "    - The boundary of classifier is ${\\{(x,y)âˆ£\\sigma(g(x,y;\\theta))=0.5}\\}$.\n",
    "    - Plot the boundary by contour function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.contour(X,Y,sigmoids1,levels = [0.5],colors='red')\n",
    "plt.contour(X,Y,sigmoids2,levels = [0.5],colors='green')\n",
    "plt.contour(X,Y,sigmoids3,levels = [0.5],colors='blue')\n",
    "plt.scatter(pointX0, pointY0, c='b')\n",
    "plt.scatter(pointX1, pointY1, c='r')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('6. The optimal classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
