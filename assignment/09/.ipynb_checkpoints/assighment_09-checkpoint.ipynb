{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings#ignore warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "file_data   = \"mnist.csv\"\n",
    "handle_file = open(file_data, \"r\")\n",
    "data        = handle_file.readlines() #list[10,000]\n",
    "handle_file.close()\n",
    "\n",
    "size_row    = 28    # height of the image\n",
    "size_col    = 28    # width of the image\n",
    "\n",
    "num_image   = len(data)\n",
    "count       = 0     # count for the number of images\n",
    "\n",
    "num_training = 6000\n",
    "num_testing = 4000\n",
    "\n",
    "# normalize the values of the input data to be [0, 1]\n",
    "def normalize(data):\n",
    "    \n",
    "    data_normalized = (data - min(data)) / (max(data) - min(data))\n",
    "    \n",
    "    return(data_normalized)\n",
    "\n",
    "# make a matrix each column of which represents an images in a vector form\n",
    "list_image    = np.empty((size_row * size_col, num_image), dtype=float)    #784*10,000\n",
    "list_label    = np.empty(num_image, dtype=int) #list[10,000]\n",
    "\n",
    "for line in data:\n",
    "\n",
    "    line_data   = line.split(',') #list[1+784]\n",
    "    label       = line_data[0]\n",
    "    im_vector   = np.asfarray(line_data[1:]) #list[784]\n",
    "    im_vector   = normalize(im_vector)\n",
    "\n",
    "    list_label[count]       = label\n",
    "    list_image[:, count]    = im_vector\n",
    "\n",
    "    count += 1\n",
    "    \n",
    "count = 0\n",
    "label_vectors = np.empty((10, num_image), dtype=float)\n",
    "zero_vector = np.zeros(10, dtype=float)\n",
    "\n",
    "for l in list_label:\n",
    "    la_vector    = zero_vector\n",
    "    la_vector[l] = 1\n",
    "    \n",
    "    label_vectors[:, count] = la_vector\n",
    "    count += 1\n",
    "    \n",
    "    \n",
    "image_training = list_image[:, 0:num_training]         #784*6,000\n",
    "image_testing  = list_image[:, num_training:num_image] #784*4,000\n",
    "\n",
    "list_label_training = list_label[0:num_training]         #6,000\n",
    "list_label_testing  = list_label[num_training:num_image] #4,000\n",
    "\n",
    "label_training = label_vectors[:, 0:num_training]         #10*6,000\n",
    "label_testing  = label_vectors[:, num_training:num_image] #10*4,000\n",
    "\n",
    "bias_training = np.ones((1, num_training), dtype=int)\n",
    "bias_testing  = np.ones((1, num_testing), dtype=int)\n",
    "\n",
    "image_training = np.concatenate((bias_training, image_training), axis=0)#785*6,000\n",
    "image_testing  = np.concatenate((bias_testing, image_testing), axis=0)#785*4,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(matrix):\n",
    "    return 1 / (1 + np.exp(-matrix))\n",
    "\n",
    "def loss(h, label, m):\n",
    "    return -(np.sum((label*np.log(h))+(1-label)*np.log(1-h)))/m\n",
    "\n",
    "def forward_prop(u, v, w, x, m):\n",
    "    #y  = prop_y(u, x, v)\n",
    "    bias = np.ones((1,m))\n",
    "    \n",
    "    y_ = u.dot(x)\n",
    "    y  = sigmoid(y_)\n",
    "    y  = np.concatenate((bias, y), axis=0)\n",
    "\n",
    "    z_ = v.dot(y)\n",
    "    z  = sigmoid(z_)\n",
    "    z  = np.concatenate((bias, z), axis=0)\n",
    "    \n",
    "    h_ = w.dot(z)\n",
    "    h  = sigmoid(h_)\n",
    "    \n",
    "    return y, z, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_4(h, label):\n",
    "    return h-label\n",
    "\n",
    "def delta_3(d4, w, z):\n",
    "    diff1 = (w.T).dot(d4)\n",
    "    diff2 = z * (1 - z)\n",
    "    return diff1*diff2\n",
    "\n",
    "def delta_2(d3, v, y):\n",
    "    bias = np.ones((1, 197))\n",
    "    v = np.concatenate((bias, v), axis=0)\n",
    "    diff1 = (v.T).dot(d3)\n",
    "    diff2 = y * (1 - y)\n",
    "    return diff1*diff2\n",
    "\n",
    "def partial_w(d4, z, m):\n",
    "    return (d4.dot(z.T))/m\n",
    "    \n",
    "def partial_v(d3, y, m):\n",
    "    return (d3.dot(y.T))/m\n",
    "\n",
    "def partial_u(d2, x, m):\n",
    "    return (d2.dot(x.T))/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(h, m):\n",
    "    \n",
    "    pred_list = np.empty(m, dtype=int)\n",
    "    for i in range(m):\n",
    "        max = 0\n",
    "        idx = 0\n",
    "        for j in range(10):\n",
    "            if max < h[j, i]:\n",
    "                max = h[j, i]\n",
    "                idx = j\n",
    "        pred_list[i] = idx\n",
    "        \n",
    "    return pred_list\n",
    "\n",
    "def getAccuracy(h, label, m):\n",
    "    \n",
    "    correct = 0\n",
    "    count = 0\n",
    "    pred = predict(h, m)\n",
    "    #for i in range(m):\n",
    "    for item in pred:\n",
    "        comp = label[count]\n",
    "        if item == comp:\n",
    "            correct += 1\n",
    "        count += 1\n",
    "    accuracy = correct / m\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign random values to weights(u, v, w)\n",
    "u = np.empty((196, 785))\n",
    "for i in range(785):\n",
    "    u[:, i] = np.random.normal(0.0, 1, size = 196)\n",
    "    \n",
    "v = np.empty((49, 197))\n",
    "for i in range(197):\n",
    "    v[:, i] = np.random.normal(0.0, 1, size = 49)\n",
    "    \n",
    "w = np.empty((10, 50))\n",
    "for i in range(50):\n",
    "    w[:, i] = np.random.normal(0.0, 1, size = 10)\n",
    "    \n",
    "learning_rate = 1\n",
    "t         = 0\n",
    "iteration = list()\n",
    "temp_loss = 0 \n",
    "temp_loss_test = 0\n",
    "train_loss_list = list()\n",
    "test_loss_list  = list()\n",
    "\n",
    "accuracy_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 18.689181960194613 \n",
      "\n",
      "1 0.4151759451836027 \n",
      "\n",
      "2 0.2827692890826933 \n",
      "\n",
      "3 0.271073854201593 \n",
      "\n",
      "4 0.2634036745169394 \n",
      "\n",
      "5 0.25756413524660793 \n",
      "\n",
      "6 0.25273277911664394 \n",
      "\n",
      "7 0.24852395054920154 \n",
      "\n",
      "8 0.24473094835903803 \n",
      "\n",
      "9 0.241232358968753 \n",
      "\n",
      "10 0.23795183448112667 \n",
      "\n",
      "11 0.23483872261111083 \n",
      "\n",
      "12 0.23185791970692296 \n",
      "\n",
      "13 0.22898419930103947 \n",
      "\n",
      "14 0.2261988726749479 \n",
      "\n",
      "15 0.22348773530819846 \n",
      "\n",
      "16 0.22083975590606356 \n",
      "\n",
      "17 0.21824621117132872 \n",
      "\n",
      "18 0.21570009736871198 \n",
      "\n",
      "19 0.21319571903770979 \n",
      "\n",
      "20 0.21072839449363417 \n",
      "\n",
      "21 0.20829424064123322 \n",
      "\n",
      "22 0.20589001335274712 \n",
      "\n",
      "23 0.20351298807848311 \n",
      "\n",
      "24 0.20116087061538554 \n",
      "\n",
      "25 0.1988317311782443 \n",
      "\n",
      "26 0.19652395711175594 \n",
      "\n",
      "27 0.19423622088209258 \n",
      "\n",
      "28 0.19196746089904335 \n",
      "\n",
      "29 0.18971687339856055 \n",
      "\n",
      "30 0.18748391408094806 \n",
      "\n",
      "31 0.18526830858319707 \n",
      "\n",
      "32 0.18307007114894622 \n",
      "\n",
      "33 0.18088953112335404 \n",
      "\n",
      "34 0.17872736694401298 \n",
      "\n",
      "35 0.17658464734593557 \n",
      "\n",
      "36 0.17446287926801737 \n",
      "\n",
      "37 0.1723640615829853 \n",
      "\n",
      "38 0.17029074312044368 \n",
      "\n",
      "39 0.1682460824514199 \n",
      "\n",
      "40 0.16623390563075177 \n",
      "\n",
      "41 0.1642587562513995 \n",
      "\n",
      "42 0.16232593006658178 \n",
      "\n",
      "43 0.16044148394017024 \n",
      "\n",
      "44 0.15861220633271486 \n",
      "\n",
      "45 0.15684553440152013 \n",
      "\n",
      "46 0.15514940189275953 \n",
      "\n",
      "47 0.15353200351663146 \n",
      "\n",
      "48 0.1520014667267093 \n",
      "\n",
      "49 0.15056543212594978 \n",
      "\n",
      "50 0.1492305596110454 \n",
      "\n",
      "51 0.1480019975802954 \n",
      "\n",
      "52 0.14688287364084796 \n",
      "\n",
      "53 0.14587388101781829 \n",
      "\n",
      "54 0.14497303778744633 \n",
      "\n",
      "55 0.1441756803596213 \n",
      "\n",
      "56 0.14347471722278263 \n",
      "\n",
      "57 0.14286112098214607 \n",
      "\n",
      "58 0.1423245891367327 \n",
      "\n",
      "59 0.14185427240351545 \n",
      "\n",
      "60 0.141439463431734 \n",
      "\n",
      "61 0.14107015910465273 \n",
      "\n",
      "62 0.1407374470107637 \n",
      "\n",
      "63 0.14043370758837342 \n",
      "\n",
      "64 0.14015265599924723 \n",
      "\n",
      "65 0.1398892659404985 \n",
      "\n",
      "66 0.13963962160377633 \n",
      "\n",
      "67 0.1394007379985662 \n",
      "\n",
      "68 0.139170379075954 \n",
      "\n",
      "69 0.1389468918059416 \n",
      "\n",
      "70 0.1387290650259369 \n",
      "\n",
      "71 0.13851601527669505 \n",
      "\n",
      "72 0.1383070979600418 \n",
      "\n",
      "73 0.13810184023672092 \n",
      "\n",
      "74 0.13789989152755847 \n",
      "\n",
      "75 0.1377009876839503 \n",
      "\n",
      "76 0.13750492543106602 \n",
      "\n",
      "77 0.13731154436052695 \n",
      "\n",
      "78 0.13712071435578985 \n",
      "\n",
      "79 0.13693232687233867 \n",
      "\n",
      "80 0.13674628892210966 \n",
      "\n",
      "81 0.1365625189340724 \n",
      "\n",
      "82 0.13638094390061076 \n",
      "\n",
      "83 0.13620149741866752 \n",
      "\n",
      "84 0.1360241183260257 \n",
      "\n",
      "85 0.13584874974745953 \n",
      "\n",
      "86 0.13567533841794857 \n",
      "\n",
      "87 0.1355038341988251 \n",
      "\n",
      "88 0.13533418970603764 \n",
      "\n",
      "89 0.1351663600426267 \n",
      "\n",
      "90 0.13500030257256873 \n",
      "\n",
      "91 0.13483597674854106 \n",
      "\n",
      "92 0.1346733439574149 \n",
      "\n",
      "93 0.13451236739269773 \n",
      "\n",
      "94 0.13435301193567192 \n",
      "\n",
      "95 0.1341952440510857 \n",
      "\n",
      "96 0.13403903169012893 \n",
      "\n",
      "97 0.13388434419809161 \n",
      "\n",
      "98 0.13373115222832402 \n",
      "\n",
      "99 0.1335794276613798 \n",
      "\n",
      "100 0.13342914352525864 \n",
      "\n",
      "101 0.13328027392137062 \n",
      "\n",
      "102 0.13313279395332875 \n",
      "\n",
      "103 0.13298667965292324 \n",
      "\n",
      "104 0.13284190791839876 \n",
      "\n",
      "105 0.13269845644349684 \n",
      "\n",
      "106 0.13255630365658536 \n",
      "\n",
      "107 0.1324154286593698 \n",
      "\n",
      "108 0.13227581116449852 \n",
      "\n",
      "109 0.13213743144004791 \n",
      "\n",
      "110 0.132000270253118 \n",
      "\n",
      "111 0.13186430881557717 \n",
      "\n",
      "112 0.1317295287323699 \n",
      "\n",
      "113 0.13159591195166936 \n",
      "\n",
      "114 0.13146344072063026 \n",
      "\n",
      "115 0.13133209753780908 \n",
      "\n",
      "116 0.1312018651137144 \n",
      "\n",
      "117 0.13107272633376021 \n",
      "\n",
      "118 0.13094466422010795 \n",
      "\n",
      "119 0.13081766190259292 \n",
      "\n",
      "120 0.1306917025871766 \n",
      "\n",
      "121 0.1305667695340055 \n",
      "\n",
      "122 0.13044284603136277 \n",
      "\n",
      "123 0.13031991538088095 \n",
      "\n",
      "124 0.13019796087975857 \n",
      "\n",
      "125 0.1300769658096585 \n",
      "\n",
      "126 0.12995691342659865 \n",
      "\n",
      "127 0.12983778695569048 \n",
      "\n",
      "128 0.12971956958734174 \n",
      "\n",
      "129 0.12960224447549504 \n",
      "\n",
      "130 0.1294857947398516 \n",
      "\n",
      "131 0.12937020346884642 \n",
      "\n",
      "132 0.12925545372342967 \n",
      "\n",
      "133 0.12914152854570157 \n",
      "\n",
      "134 0.12902841096569198 \n",
      "\n",
      "135 0.1289160840091538 \n",
      "\n",
      "136 0.12880453070887332 \n",
      "\n",
      "137 0.12869373411228544 \n",
      "\n",
      "138 0.12858367729536085 \n",
      "\n",
      "139 0.1284743433688512 \n",
      "\n",
      "140 0.12836571549119344 \n",
      "\n",
      "141 0.12825777687778986 \n",
      "\n",
      "142 0.12815051080944143 \n",
      "\n",
      "143 0.1280439006427007 \n",
      "\n",
      "144 0.12793792981692173 \n",
      "\n",
      "145 0.1278325818620335 \n",
      "\n",
      "146 0.12772784040388963 \n",
      "\n",
      "147 0.12762368917119588 \n",
      "\n",
      "148 0.12752011199822916 \n",
      "\n",
      "149 0.12741709282845243 \n",
      "\n",
      "150 0.1273146157179129 \n",
      "\n",
      "151 0.12721266483350202 \n",
      "\n",
      "152 0.1271112244555361 \n",
      "\n",
      "153 0.12701027897463474 \n",
      "\n",
      "154 0.12690981288888928 \n",
      "\n",
      "155 0.12680981080234327 \n",
      "\n",
      "156 0.12671025741777522 \n",
      "\n",
      "157 0.12661113753199638 \n",
      "\n",
      "158 0.12651243602863535 \n",
      "\n",
      "159 0.1264141378700125 \n",
      "\n",
      "160 0.1263162280880273 \n",
      "\n",
      "161 0.1262186917739704 \n",
      "\n",
      "162 0.12612151406688385 \n",
      "\n",
      "163 0.12602468014113927 \n",
      "\n",
      "164 0.12592817519450786 \n",
      "\n",
      "165 0.1258319844308019 \n",
      "\n",
      "166 0.12573609304681033 \n",
      "\n",
      "167 0.12564048621488685 \n",
      "\n",
      "168 0.12554514906414396 \n",
      "\n",
      "169 0.12545006666209144 \n",
      "\n",
      "170 0.12535522399442678 \n",
      "\n",
      "171 0.1252606059428653 \n",
      "\n",
      "172 0.12516619726215145 \n",
      "\n",
      "173 0.12507198255582908 \n",
      "\n",
      "174 0.12497794624961654 \n",
      "\n",
      "175 0.12488407256410941 \n",
      "\n",
      "176 0.12479034548583762 \n",
      "\n",
      "177 0.12469674873628807 \n",
      "\n",
      "178 0.12460326573863997 \n",
      "\n",
      "179 0.12450987958458541 \n",
      "\n",
      "180 0.12441657299834218 \n",
      "\n",
      "181 0.12432332830020741 \n",
      "\n",
      "182 0.1242301273673328 \n",
      "\n",
      "183 0.12413695159636272 \n",
      "\n",
      "184 0.12404378186470667 \n",
      "\n",
      "185 0.12395059849089571 \n",
      "\n",
      "186 0.12385738119935523 \n",
      "\n",
      "187 0.1237641090859045 \n",
      "\n",
      "188 0.1236707605881591 \n",
      "\n",
      "189 0.12357731346206156 \n",
      "\n",
      "190 0.1234837447673353 \n",
      "\n",
      "191 0.1233900308638362 \n",
      "\n",
      "192 0.12329614742300857 \n",
      "\n",
      "193 0.12320206945897191 \n",
      "\n",
      "194 0.12310777138212645 \n",
      "\n",
      "195 0.12301322708295839 \n",
      "\n",
      "196 0.12291841005081348 \n",
      "\n",
      "197 0.12282329353431509 \n",
      "\n",
      "198 0.12272785075175428 \n",
      "\n",
      "199 0.12263205515767818 \n",
      "\n",
      "200 0.12253588077435988 \n",
      "\n",
      "201 0.12243930259330761 \n",
      "\n",
      "202 0.12234229705354696 \n",
      "\n",
      "203 0.12224484259983942 \n",
      "\n",
      "204 0.12214692031933529 \n",
      "\n",
      "205 0.12204851465316834 \n",
      "\n",
      "206 0.12194961417151255 \n",
      "\n",
      "207 0.12185021239339557 \n",
      "\n",
      "208 0.12175030862467623 \n",
      "\n",
      "209 0.12164990877674373 \n",
      "\n",
      "210 0.121549026122398 \n",
      "\n",
      "211 0.12144768193344233 \n",
      "\n",
      "212 0.12134590594288375 \n",
      "\n",
      "213 0.12124373657286414 \n",
      "\n",
      "214 0.12114122087389644 \n",
      "\n",
      "215 0.12103841413325687 \n",
      "\n",
      "216 0.12093537913105577 \n",
      "\n",
      "217 0.12083218504344213 \n",
      "\n",
      "218 0.12072890602331682 \n",
      "\n",
      "219 0.12062561951877304 \n",
      "\n",
      "220 0.12052240440863546 \n",
      "\n",
      "221 0.1204193390602987 \n",
      "\n",
      "222 0.12031649941508603 \n",
      "\n",
      "223 0.12021395720755036 \n",
      "\n",
      "224 0.12011177840889956 \n",
      "\n",
      "225 0.12001002196262474 \n",
      "\n",
      "226 0.11990873885364214 \n",
      "\n",
      "227 0.11980797152495065 \n",
      "\n",
      "228 0.11970775362919775 \n",
      "\n",
      "229 0.11960811008471298 \n",
      "\n",
      "230 0.11950905739093108 \n",
      "\n",
      "231 0.11941060415075049 \n",
      "\n",
      "232 0.11931275174789686 \n",
      "\n",
      "233 0.11921549512865745 \n",
      "\n",
      "234 0.11911882364518478 \n",
      "\n",
      "235 0.11902272192364381 \n",
      "\n",
      "236 0.11892717072953442 \n",
      "\n",
      "237 0.11883214780990323 \n",
      "\n",
      "238 0.11873762869806787 \n",
      "\n",
      "239 0.11864358747220072 \n",
      "\n",
      "240 0.1185499974634097 \n",
      "\n",
      "241 0.11845683191046555 \n",
      "\n",
      "242 0.11836406456136216 \n",
      "\n",
      "243 0.11827167022145456 \n",
      "\n",
      "244 0.11817962524857858 \n",
      "\n",
      "245 0.11808790799405389 \n",
      "\n",
      "246 0.11799649918877422 \n",
      "\n",
      "247 0.11790538227143738 \n",
      "\n",
      "248 0.11781454365545953 \n",
      "\n",
      "249 0.11772397293019389 \n",
      "\n",
      "250 0.11763366299212968 \n",
      "\n",
      "251 0.11754361010114531 \n",
      "\n",
      "252 0.11745381385861423 \n",
      "\n",
      "253 0.11736427710488631 \n",
      "\n",
      "254 0.11727500573689942 \n",
      "\n",
      "255 0.11718600844807348 \n",
      "\n",
      "256 0.11709729639709296 \n",
      "\n",
      "257 0.11700888281455422 \n",
      "\n",
      "258 0.11692078256008652 \n",
      "\n",
      "259 0.11683301164477075 \n",
      "\n",
      "260 0.116745586735901 \n",
      "\n",
      "261 0.11665852466168956 \n",
      "\n",
      "262 0.11657184193403074 \n",
      "\n",
      "263 0.11648555430483327 \n",
      "\n",
      "264 0.11639967637101567 \n",
      "\n",
      "265 0.11631422123829922 \n",
      "\n",
      "266 0.11622920025206701 \n",
      "\n",
      "267 0.11614462279866206 \n",
      "\n",
      "268 0.1160604961774513 \n",
      "\n",
      "269 0.11597682554067174 \n",
      "\n",
      "270 0.11589361389513922 \n",
      "\n",
      "271 0.11581086215829875 \n",
      "\n",
      "272 0.11572856925950477 \n",
      "\n",
      "273 0.11564673227697396 \n",
      "\n",
      "274 0.11556534660082649 \n",
      "\n",
      "275 0.11548440611335024 \n",
      "\n",
      "276 0.11540390337810054 \n",
      "\n",
      "277 0.11532382983096952 \n",
      "\n",
      "278 0.11524417596741525 \n",
      "\n",
      "279 0.11516493152120352 \n",
      "\n",
      "280 0.1150860856315015 \n",
      "\n",
      "281 0.11500762699575326 \n",
      "\n",
      "282 0.114929544007194 \n",
      "\n",
      "283 0.11485182487644208 \n",
      "\n",
      "284 0.11477445773703497 \n",
      "\n",
      "285 0.11469743073568917 \n",
      "\n",
      "286 0.11462073210788255 \n",
      "\n",
      "287 0.11454435024025061 \n",
      "\n",
      "288 0.11446827372060477 \n",
      "\n",
      "289 0.11439249137723095 \n",
      "\n",
      "290 0.11431699230868218 \n",
      "\n",
      "291 0.11424176590525892 \n",
      "\n",
      "292 0.11416680186352589 \n",
      "\n",
      "293 0.11409209019486838 \n",
      "\n",
      "294 0.11401762122927916 \n",
      "\n",
      "295 0.11394338561507716 \n",
      "\n",
      "296 0.11386937431555272 \n",
      "\n",
      "297 0.11379557860313254 \n",
      "\n",
      "298 0.11372199005177998 \n",
      "\n",
      "299 0.11364860052804598 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-cce2b2ec708b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_loss\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.000001\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mfin_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;31m#need?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mfin_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-2d7a8e63b51a>\u001b[0m in \u001b[0;36mforward_prop\u001b[0;34m(u, v, w, x, m)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0my\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0my\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    (y, z, h) = forward_prop(u, v, w, image_training, num_training)\n",
    "    if t>1 and abs(temp_loss - loss(h, label_training, num_training)) <= 0.000001:\n",
    "        fin_u = u#need?\n",
    "        fin_v = v\n",
    "        fin_w = w\n",
    "        break\n",
    "    temp_loss = loss(h, label_training, num_training)\n",
    "    train_loss_list.append(temp_loss)\n",
    "    iteration.append(t)\n",
    "    \n",
    "    (y_test, z_test, h_test) = forward_prop(u, v, w, image_testing, num_testing)\n",
    "    temp_loss_test = loss(h, label_testing, num_testing)\n",
    "    test_loss_list.append(t)\n",
    "    \n",
    "    d4 = delta_4(h, label_training)\n",
    "    d3 = delta_3(d4, w, z)\n",
    "    d2 = delta_2(d3, v, y)\n",
    "    \n",
    "    p_w = partial_w(d4, z, num_training)\n",
    "    temp_p_v = partial_v(d3, y, num_training)\n",
    "    temp_p_u = partial_u(d2, image_training, num_training)\n",
    "    \n",
    "    p_v = temp_p_v[1:50, :]\n",
    "    p_u = temp_p_u[1:197, :]\n",
    "        \n",
    "    w = w - learning_rate * p_w\n",
    "    v = v - learning_rate * p_v\n",
    "    u = u - learning_rate * p_u\n",
    "    \n",
    "    fin_accuracy = getAccuracy(h, list_label_training, num_training)\n",
    "    \n",
    "    print(t, temp_loss, '\\n')\n",
    "    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(iteration1, train_loss_list, c = 'blue')\n",
    "plt.plot(iteration2, test_loss_list, c = 'red')\n",
    "plt.legend(['training error', 'testing error'])\n",
    "plt.xlabel('t : iteration')\n",
    "plt.ylabel('cost value')\n",
    "plt.title('1. The loss curve')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
